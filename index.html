<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="This paper proposes a novel framework for human-robot collaboration (HRC) that integrates Robot Assistance Primitives (RAPs) with force-field guidance to enable versatile and safe shared-task collaboration in unstructured environments.">
  <meta name="keywords" content="Robot Assistance Primitives, HRC, shared tasks, impedance control, force-field guidance, AR-HMD">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Robot Assistance Primitives with Force-Field Guidance</title>

  <!-- (Optional) Google Analytics: remove or replace if not needed -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=YOUR-ID"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'YOUR-ID');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h1 class="title is-1 publication-title">Robot Assistance Primitives with Force-Field Guidance</h1>
      <h2 class="subtitle is-5 has-text-grey">
            Published in <em>Robotics and Computer-Integrated Manufacturing</em> (2025)
      </h2>
      <p class="subtitle is-5 publication-authors">
        <a href="#">Sophokles Ktistakis</a><sup>1*</sup>, 
        <a href="#">Lucas Gimeno</a><sup>1</sup>, 
        <a href="#">Fatima-Zahra Laftissi</a><sup>2</sup>, 
        <a href="#">Alexis Hoss</a><sup>2</sup>, 
        <a href="#">Antonio De Donno</a><sup>2</sup>, 
        <a href="#">Mirko Meboldt</a><sup>1</sup>
      </p>
      <p class="publication-affiliations">
        <sup>1</sup>ETH Zurich &nbsp;|&nbsp; <sup>2</sup>Accenture Labs
      </p>
      
    

                   <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://www.sciencedirect.com/science/article/pii/S0736584525001152#fig0011" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                    </div>
                   </div> 
       

<section class="hero teaser">
  <div class="hero-body">
    <div class="container has-text-centered">
      <!-- TODO: Add your project teaser video or image here -->
      <div class="notification is-light">
        <em>Teaser video placeholder</em>
      </div>
    </div>
  </div>
</section>

<section class="section" id="Abstract">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Abstract</h2>
    <div class="content has-text-justified">
      <p>This paper proposes a novel framework for human-robot collaboration (HRC) that addresses the critical need for robots to effectively collaborate with humans on shared tasks within unstructured and dynamic environments. While prior research focused on safety-related aspects, such as collision avoidance in shared workspaces, the task-oriented aspects of human-robot collaboration remain largely underexplored. To address this gap, our framework introduces Robot Assistance Primitives (RAPs), low-level robot actions that integrate both safety and task-related behaviors, enabling the robot to function as a collaborative “third hand” across physical and contactless interactions. A key component is an extension of impedance control with virtual force fields, unifying task guidance and collision avoidance. We leverage a state-of-the-art visual perception pipeline for real-time 3D scene understanding and an AR-HMD interface for multimodal task programming. We validate feasibility through technical experiments and conduct a user study on collaborative soldering and assembly, demonstrating significant improvements in efficiency and reduced cognitive load.</p>
    </div>
  </div>
</section>

<section class="section" id="Methods">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Methods</h2>
    <div class="content" style="text-align: center;">
      <img src="static/images/1-s2.0-S0736584525001152-gr2_lrg.jpg" 
           alt="Figure 1: System Overview" 
           style="max-width: 100%; height: auto; margin-top: 10px;">
      <p style="margin-top: 15px;">
        Our framework integrates three core components: (1) a real-time RGB-D visual perception pipeline for 3D scene understanding and object detection, (2) a multimodal AR interface for intuitive task programming via gaze, gestures, and speech, and (3) an impedance-based control scheme augmented with virtual force fields to unify task guidance and collision avoidance. These elements enable the execution of Robot Assistance Primitives (RAPs), versatile low-level actions that support both physical and contactless collaboration in unstructured environments</p>
    </div>
  </div>
</section>

<section class="section" id="Experiments">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Experiments</h2>
    <div class="content" style="text-align: center;">
      <img src="static/images/experiments.jpg" 
           alt="Experiment Overview" 
           style="max-width: 100%; height: auto; margin-top: 10px;">
      <p style="margin-top: 15px;">
       We conducted a user study with 22 participants performing a collaborative soldering and assembly task, comparing our robotic assistance framework to a manual baseline. The experimental group used Robot Assistance Primitives (RAPs) for PCB handling and assembly, while the control group relied on table-mounted fixtures. We evaluated task completion time, cognitive load (NASA TLX), user experience, and system safety across repeated trials to assess efficiency, usability, and collision avoidance.
    </div>
  </div>
</section>


<section class="section" id="Results">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Results</h2>
    <div class="content has-text-centered">
      
      <!-- First Image: Net Time per Run -->
      <figure>
        <img src="static/images/res_net_time.jpg" 
             alt="Net Time per Run"
             style="max-width: 90%; height: auto; margin-bottom: 20px;">
        <figcaption style="margin-top: 10px; font-size: 0.9em; color: #555;">
          Figure: Net task execution time across three runs for experimental (Robot) and control groups.
        </figcaption>
      </figure>

      <!-- Second Image: NASA TLX Results -->
      <figure>
        <img src="static/images/res_tlx.jpg" 
             alt="NASA TLX Results"
             style="max-width: 90%; height: auto; margin-bottom: 20px;">
        <figcaption style="margin-top: 10px; font-size: 0.9em; color: #555;">
          Figure: Average NASA TLX scores comparing first and last runs for both groups.
        </figcaption>
      </figure>

      <p>
        The experimental group improved task completion time by 55.4% after three runs, although the manual control group remained faster overall. NASA TLX analysis showed a statistically significant reduction in physical demand (p = 0.012), along with lower frustration and higher perceived performance for the experimental group after repeated use. Participants generally found the system engaging and helpful despite some technical issues, and safety checks confirmed minimal collisions and consistent distance maintenance during handovers.
      </p>
    </div>
  </div>
</section>

  

<section class="section" id="Discussion and Conclusion">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Discussion and Conclusion</h2>
    <div class="content">
      <p>
        The proposed RAP framework enables flexible, task-oriented collaboration by integrating impedance control with force-field guidance and real-time 3D perception, improving safety and reducing cognitive and physical effort during shared tasks. While user studies confirmed its effectiveness and positive user experience, challenges such as slower execution, gesture and speech recognition errors, AR HMD fatigue, and occasional robot instability remain. Future work should address these limitations through hybrid control, improved intent inference, and faster, more predictable motions to fully unlock the framework’s potential for practical human-robot collaboration.
      </p>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">BibTeX</h2>
    <pre style="text-align: left; margin-left: 0;">
      <code>@article{ktistakis2025robot,
  author    = {Sophokles Ktistakis and Lucas Gimeno and Fatima-Zahra Laftissi and Alexis Hoss and Antonio {De Donno} and Mirko Meboldt},
  title     = {Robot assistance primitives with force-field guidance for shared task collaboration},
  journal   = {Robotics and Computer-Integrated Manufacturing},
  volume    = {96},
  year      = {2025},
  pages     = {103061},
  issn      = {0736-5845},     
  doi       = {https://doi.org/10.1016/j.rcim.2025.103061},
}
</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="content has-text-centered">
    <p>This website is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> license.</p>
  </div>
</footer>


  
</body>
</html>
